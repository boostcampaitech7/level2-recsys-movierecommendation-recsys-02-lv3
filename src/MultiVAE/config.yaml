# config.yaml
# 직접 하이퍼파라미터를 추가하여 관리할 수 있습니다.

memo: |-
    남겨둘 메모가 있다면 여기에.
    여러 줄로도 작성 가능

checkpoint: 'saved/checkpoint/MultiVAE.pt'   # 예측 시 불러올 모델 경로
seed: 1111            # 시드 고정
predict: False
device: cuda          # 가능한 값 : cpu, cuda, mps
model: MultiVAE       # 모델 선택
run_name: 'choi : MultiVAE'
wandb: True                                # wandb 사용 여부
wandb_project: 'lv2.movie_recommendation'  # wandb 프로젝트 이름

model_args:     # model에 해당하는 파라미터만 실질적으로 사용됩니다.
    MultiVAE:
        p_dims: [200, 400]         



dataset:
    data_path: ./data/train/    # 데이터셋 경로
    ratio: 0.1                  # Train / Vaildation / Test split
    test_prop : 0.2

dataloader:
    data_path: ./choi/level2-recsys-movierecommendation-recsys-02-lv3/src/    # 데이터 load 경로(개인 폴더 경로로 설정해야함)
    batch_size: 250         # 배치 사이즈
    shuffle: True           # 학습 데이터 셔플 여부
    num_workers: 0          # 멀티프로세서 수. 0: 메인프로세서만 사용

optimizer:
    type: Adam      # 사용가능한 optimizer: torch.optim.Optimizer 클래스 (https://pytorch.org/docs/stable/optim.html#algorithms)
    args:           # 사용하고자 하는 클래스의 파라미터를 참고하여 추가해주시면 되며, 관계가 없는 파라미터는 무시됩니다.
        lr: 0.0008970328285854382                # 예) 모든 옵티마이저에서 사용되는 학습률
        weight_decay: 1.9476641809876873e-05     # 예) Adam 등 / L2 정규화 가중치
        amsgrad: False                           # 예) Adam 등 / amsgrad 사용 여부

other_params:
    args:
        total_anneal_steps: 300000
        anneal_cap: 0.1
        log_interval: 100



lr_scheduler:
    use: False                  # True: 사용 / False: 사용하지 않음 (단, valid_ratio가 0일 경우 validation set이 없어 사용 불가)
    type: ReduceLROnPlateau     # 사용가능한 lr_scheduler: torch.optim.lr_scheduler 클래스 (https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)
    args:                       # 사용하고자 하는 클래스의 파라미터를 참고하여 추가해주시면 되며, 관계가 없는 파라미터는 무시됩니다.
        mode: 'min'             # 예) ReduceLROnPlateau / 'min' 또는 'max'
        factor: 0.1             # 예) ReduceLROnPlateau / 학습률 감소 비율
        step_size: 10           # 예) StepLR / 학습률 감소 주기 (필수)
        gamma: 0.1              # 예) StepLR 등 / 학습률 감소 비율


# 전체 경로 수정
# 기본
# dir : saved/~

train:
    epochs: 20                          # 학습 에폭 수
    log_dir: saved/MultiVAE/log                  # 로그 저장 경로
    ckpt_dir: saved/MultiVAE/checkpoint    # 모델 저장 경로
    submit_dir: saved/MultiVAE/submit            # 예측 저장 경로
    save_best_model: True               # True: val_loss가 최소인 모델 저장 / False: 모든 모델 저장
    resume: False                               # 이어서 학습할 경우 True
    resume_path: saved/checkpoint/MultiVAE.pt    # 이어서 학습할 모델 경로