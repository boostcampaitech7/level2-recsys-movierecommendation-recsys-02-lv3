program: ./choi/level2-recsys-movierecommendation-recsys-02-lv3/main.py                    # 실행할 파일
name: "Sweep MultiVAE"              # Sweep + {model_name}
project: lv2.movie_recommendation    # 프로젝트 명
method: bayes                       # 파라미터 method
metric:
    goal: minimize
    name: 'Valid loss'
description: |-
    MultiVAE wandb Sweep

parameters:
    config:         # configuration 파일 경로. 비워두면 됩니다.
        value: ''
    predict:
        value: False
    checkpoint:
        value: 'saved/checkpoint/~~.pt'
    device:         # 가능한 값 : cpu, cuda, mps
        value: cuda
    model:          # 모델 선택
        value: MultiVAE
    wandb:          # wandb 사용 여부
        value: True
    wandb_project:  # wandb 프로젝트 이름
        value: 'lv2.movie_recommendation'
    run_name:       # wandb 실행 이름. 빈 문자열일 경우 자동 생성
        value: ''
    seed:           # 시드 고정 (튜닝)
        values: [0, 42, 1111]

    model_args:     # model에 해당하는 파라미터만 실질적으로 사용됩니다.
        parameters:
            MultiVAE:                 # 모델 이름
                parameters:
                    p_dims:
                        values:         
                            - [200, 600]
                            - [200, 400]
                            - [400, 600]
                            - [200, 400, 600]
    dataset:
        parameters:
            data_path:            # 데이터셋 경로
                value: ./data/train/
            ratio:                # Train / Vaildation / Test split
                value: 0.1
            test_prop:
                value: 0.2

    dataloader:
        parameters:
            data_path:          # 데이터로더 경로
                value: ./choi/level2-recsys-movierecommendation-recsys-02-lv3/src/data/ 
            batch_size:         # 배치 사이즈 (튜닝)
                values: [250, 500, 1000]
            shuffle:            # 학습 데이터 셔플 여부
                value: True
            num_workers:        # 멀티프로세서 수. 0: 메인프로세서만 사용
                value: 0

    optimizer:
        parameters:
            type:         # 사용가능한 optimizer: torch.optim.Optimizer 클래스 (https://pytorch.org/docs/stable/optim.html#algorithms)
                value: Adam
            args:           # 사용하고자 하는 클래스의 파라미터를 참고하여 추가해주시면 되며, 관계가 없는 파라미터는 무시됩니다.
                parameters:
                    lr:                 # 예) 모든 옵티마이저에서 사용되는 학습률 (튜닝)
                        min: 1e-4
                        max: 1e-3
                    weight_decay:       # 예) Adam 등 / L2 정규화 가중치 (튜닝)
                        min: 0
                        max: 1e-4
                    amsgrad:            # 예) Adam 등 / amsgrad 사용 여부 (튜닝)
                        value: False

    other_params:
        parameters:
            args:
                parameters:
                    total_anneal_steps:  
                        values: [100000, 200000, 300000]
                    anneal_cap:
                        values: [0.1, 0.2, 0.5]
                    log_interval:
                        value: 100    


        
    lr_scheduler:
        parameters:
            use:                        # True: 사용 / False: 사용하지 않음 (튜닝)
                value: False
            type:                       # 사용가능한 lr_scheduler: torch.optim.lr_scheduler 클래스 (튜닝)
                values: [ReduceLROnPlateau, StepLR]
            args:                       # 사용하고자 하는 클래스의 파라미터를 참고하여 추가해주시면 되며, 관계가 없는 파라미터는 무시됩니다.
                parameters:
                    mode:               # 예) ReduceLROnPlateau / 'min' 또는 'max'
                        value: min
                    factor:             # 예) ReduceLROnPlateau / 학습률 감소 비율
                        value: 0.1
                    patience:           # 예) ReduceLROnPlateau / 학습률 감소 대기 기간
                        value: 5
                    cooldown:           # 예) ReduceLROnPlateau / 학습률 감소 후 다시 학습률 감소까지 대기 기간
                        value: 1
                    step_size:          # 예) StepLR / 학습률 감소 주기 (필수)
                        value: 10
                    gamma:              # 예) StepLR 등 / 학습률 감소 비율
                        value: 0.1


    train:
        parameters:
            epochs:             # 학습 에폭 수
                value: 20
            log_dir:            # 로그 저장 경로
                value: saved/log
            ckpt_dir:           # 모델 저장 경로
                value: saved/checkpoint    
            submit_dir:         # 예측 저장 경로
                value: saved/submit
            save_best_model:    # True: val_loss가 최소인 모델 저장 / False: 모든 모델 저장
                value: True
            resume:             # 이어서 학습할 경우 True
                value: False
            resume_path:
                value: ''